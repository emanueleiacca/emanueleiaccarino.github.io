[
  {
    "objectID": "pubblications.html",
    "href": "pubblications.html",
    "title": "pubblications",
    "section": "",
    "text": "Deep Reinforcement Learning for Incomplete Information Card Games (BlackJack and Poker)\n\n\nApplication of RL and DRL algorithms to strategic decision-making in Blackjack and Poker, exploring uncertainty and incomplete information.\n\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nBreast Cancer Classification with Deep Learning\n\n\nA deep learning pipeline for classifying mammographic images from the CBIS dataset, leveraging preprocessing and transfer learning.\n\n\n\n\n\n\nDec 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnity ML Agents: Wall Jump and SoccerTwos Environment using Reinforcement Learning Techniques\n\n\nImplementation and comparative study of reinforcement learning algorithms in Unity ML environments, with case studies on WallJump and SoccerTwos.\n\n\n\n\n\n\nDec 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring and Optimizing Reinforcement Learning Algorithms in the Frozen Lake Environment\n\n\nAnalysis of RL algorithms in deterministic and stochastic Frozen Lake environments, with hyperparameter optimization via Optuna.\n\n\n\n\n\n\nOct 1, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pubblications/Frozen Lake/index.html",
    "href": "pubblications/Frozen Lake/index.html",
    "title": "Exploring and Optimizing Reinforcement Learning Algorithms in the Frozen Lake Environment",
    "section": "",
    "text": "This project evaluates reinforcement learning algorithms in the Frozen Lake environment, under both deterministic and stochastic settings.\nAlgorithms studied include Monte Carlo, Sarsa, Expected Sarsa, Q-Learning, and Double Q-Learning.\nA key focus is hyperparameter optimization with Optuna, applied to Q-Learning to improve learning efficiency.\nKey results:\n- Q-Learning and Double Q-Learning performed robustly in stochastic scenarios\n- Discount rate and learning rate were critical for convergence and stability\n- Optuna tuning enhanced Q-Learning’s efficiency across environments\n- Findings provide guidance for RL applications in grid-world tasks\nRead full-text on ResearchGate\n\n\n\nExploring deterministic and stochastic Frozen Lake environments with RL"
  },
  {
    "objectID": "pubblications/BlackJack and Poker/index.html",
    "href": "pubblications/BlackJack and Poker/index.html",
    "title": "Deep Reinforcement Learning for Incomplete Information Card Games (BlackJack and Poker)",
    "section": "",
    "text": "This thesis explores reinforcement learning (RL) and deep reinforcement learning (DRL) techniques in Blackjack and Poker, two games of incomplete information that require complex strategy under uncertainty.\nMethods include Deep Q-Networks (DQN), Counterfactual Regret Minimization (CFR), Deep Monte Carlo (DMC), and Neural Fictitious Self-Play (NFSP), evaluated for their ability to handle stochasticity, hidden information, and strategic adaptation.\nKey results:\n- DQN learns structured Blackjack strategies effectively\n- CFR and NFSP excel in Poker, adapting to incomplete information\n- DMC achieves robust convergence in high-dimensional state spaces\n- DRL methods demonstrate potential for superhuman performance in competitive card games\nRead full-text on ResearchGate\n\n\n\nDeep reinforcement learning applied to Blackjack and Poker"
  },
  {
    "objectID": "projects/SQL Parser Data Pipeline/index.html",
    "href": "projects/SQL Parser Data Pipeline/index.html",
    "title": "SQL Parser Data Pipeline",
    "section": "",
    "text": "This project introduces SQLParserDataPipeline, a Python package for parsing and interpreting complex SQL queries.\nIt was designed with BigQuery in mind but is flexible enough to adapt to other SQL dialects thanks to a parsing strategy focused on the inner query structure rather than specific SQL functions.\nCore capabilities\n- Select Clause Parsing: handles nested queries, functions, and placeholders beyond the scope of standard parsers\n- From Clause Analysis: extracts tables and aliases in medium-complexity queries\n- Unnest Transformations: identifies join types, aliases, and unique values, crucial for data pipeline design\nKey strengths\n- Outperforms standard SQL parsers on queries with nested SELECTs and functions\n- Enables clearer lineage extraction and debugging for ETL workflows\n- Provides transparent parsing results, supporting modular pipeline development\nView project on GitHub\n\n\n\nParsing complex SQL queries to support ETL pipelines and lineage tracking"
  },
  {
    "objectID": "projects/Prediction of Medical Care Abandonment in Italy/index.html",
    "href": "projects/Prediction of Medical Care Abandonment in Italy/index.html",
    "title": "Prediction of Medical Care Abandonment in Italy",
    "section": "",
    "text": "source code\n\nThis project investigates medical care abandonment in Italy using the 2019 European Health Interview Survey (EHIS).\nThe analysis combines machine learning models (multinomial logistic regression, XGBoost) with advanced techniques for feature selection, uncertainty quantification, and interpretability.\nKey methods include:\n- LASSO regression and gain-based importance for feature selection\n- Bootstrap stability analysis\n- Conformal prediction for uncertainty-aware classification\n- SHAP analysis for model interpretability\nResults show that XGBoost outperforms logistic regression, renunciation indicators (dental care, medication, mental health) are the strongest predictors, and conformal prediction provides reliable coverage. These findings highlight regional disparities and vulnerable population groups, offering actionable insights for policymakers.\n\n\n\nMachine learning applied to healthcare abandonment in Italy"
  },
  {
    "objectID": "projects/Community Detection Algorithms/index.html",
    "href": "projects/Community Detection Algorithms/index.html",
    "title": "Community Detection Algorithms — Self-Implemented and Optimized",
    "section": "",
    "text": "This project introduces a Python library for community detection, focused on performance optimization, modularity, and transparency.\nCompared to NetworkX’s implementations, the custom methods deliver faster execution and flexible configuration, while ensuring traceability through detailed logs.\nImplemented algorithms\n- Girvan-Newman: configurable betweenness calculation (BFS, Dijkstra) and component detection (recursive vs. iterative DFS)\n- Spectral Clustering: optimized eigenvalue/eigenvector computation for faster convergence\n- Louvain Method: mathematical approximation yielding extreme performance gains\nKey results\n- Girvan-Newman: execution time up to 38% faster with identical communities\n- Spectral Clustering: 92% faster than pre-implemented versions, communities preserved\n- Louvain: millions of times faster, with a small trade-off in modularity\nView project on GitHub\n\n\n\nCustom, optimized community detection methods vs. standard implementations"
  },
  {
    "objectID": "projects/Bayesian Survival Analysis of Veteran Lung Cancer Patients/index.html",
    "href": "projects/Bayesian Survival Analysis of Veteran Lung Cancer Patients/index.html",
    "title": "Bayesian Survival Analysis of Veteran Lung Cancer Patients",
    "section": "",
    "text": "source code\n\nThis project applies Bayesian survival analysis to the Veteran’s Administration Lung Cancer dataset, focusing on parametric accelerated failure time (AFT) models.\nThe work compares exponential and Weibull AFT models using MCMC estimation in JAGS, with model fit assessed via the Deviance Information Criterion (DIC).\nPosterior predictive checks and parameter recovery experiments are used to evaluate calibration and identifiability, while results are contrasted with frequentist survival models such as Cox PH and parametric AFT.\nKey findings include the superior fit of the Weibull model, significant effects of cell type on survival, and good calibration under posterior predictive validation.\n\n\n\nComparison of Bayesian posterior predictive survival curves with Kaplan–Meier estimates"
  },
  {
    "objectID": "academic.html",
    "href": "academic.html",
    "title": "my academic journey",
    "section": "",
    "text": "Bayesian Survival Analysis of Veteran Lung Cancer Patients\n\n\na Bayesian survival modeling project comparing exponential and Weibull AFT models on the Veteran’s Lung Cancer dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBreast Cancer Classification with Deep Learning\n\n\nA deep learning pipeline for classifying mammographic images from the CBIS dataset, leveraging preprocessing and transfer learning.\n\n\n\n\n\n\nDec 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nChurn Analysis on Banking Dataset\n\n\nPredicting customer churn in the banking sector using boosting algorithms, Optuna optimization, and interpretable ML techniques.\n\n\n\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunity Detection Algorithms — Self-Implemented and Optimized\n\n\nA Python library implementing and optimizing community detection algorithms (Girvan-Newman, Spectral Clustering, Louvain) with significant performance improvements over NetworkX.\n\n\n\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning for Incomplete Information Card Games (BlackJack and Poker)\n\n\nApplication of RL and DRL algorithms to strategic decision-making in Blackjack and Poker, exploring uncertainty and incomplete information.\n\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring and Optimizing Reinforcement Learning Algorithms in the Frozen Lake Environment\n\n\nAnalysis of RL algorithms in deterministic and stochastic Frozen Lake environments, with hyperparameter optimization via Optuna.\n\n\n\n\n\n\nOct 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarijuana Legalization and Opioid Mortality: A Causal Analysis Using DiD and IV Models\n\n\nEconometric study of the effects of recreational cannabis legalization on opioid and cocaine mortality, using Difference-in-Differences, IV, and Event Studies.\n\n\n\n\n\n\nJun 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrediction of Medical Care Abandonment in Italy\n\n\na research project using machine learning to identify individuals most at risk of forgoing necessary healthcare services\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSQL Parser Data Pipeline\n\n\nA Python library for parsing and interpreting complex SQL queries, designed for BigQuery workflows and adaptable to other SQL dialects.\n\n\n\n\n\n\nApr 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSnake with Curriculum Learning\n\n\nTraining an RL agent to play Snake using Stable Baselines3, with curriculum learning to progressively increase difficulty.\n\n\n\n\n\n\nSep 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnity ML Agents: Wall Jump and SoccerTwos Environment using Reinforcement Learning Techniques\n\n\nImplementation and comparative study of reinforcement learning algorithms in Unity ML environments, with case studies on WallJump and SoccerTwos.\n\n\n\n\n\n\nDec 1, 2023\n\n\n\n\n\n\n\n\n\n\n\nabout\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprojects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npubblications\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "For me, data science is not just about writing code or training models — it’s about the way you approach problems. What makes me different is not a single skill, but a combination of habits and attitudes that shape the way I work.\n\nCuriosity. I don’t stop at results, I always want to know why something behaves the way it does. That’s what keeps me digging into data until the story becomes clear.\n\nClarity. Numbers only matter if people can understand them, so I put effort into making my work transparent — whether it’s through well-documented code, clear communication, or turning messy complexity into something others can actually use.\n\nCreativity. Even though I was trained in statistics, I enjoy approaching problems from unexpected angles. Data science, for me, is also a creative act: finding new ways to ask questions and design solutions. In many ways, I bring the same mindset I have as an artist into my work with data.\n\nGrowth. This field never stands still, and neither do I. I’m motivated by the chance to explore new ideas, test emerging tools, and push myself beyond what I already know. Learning isn’t a side activity, it’s at the heart of how I keep evolving.\n\nAdaptability. My background gave me the chance to move across research and applied projects, so I’m comfortable switching between rigor and creativity depending on the context.\n\nLooking ahead, I want to grow in an industry environment where these traits can translate into real impact. For me, the next step is about sharpening my skills in practice, learning from experienced teams, and contributing to work that matters beyond the notebook."
  },
  {
    "objectID": "about.html#my-approach-soft-skills",
    "href": "about.html#my-approach-soft-skills",
    "title": "about",
    "section": "",
    "text": "For me, data science is not just about writing code or training models — it’s about the way you approach problems. What makes me different is not a single skill, but a combination of habits and attitudes that shape the way I work.\n\nCuriosity. I don’t stop at results, I always want to know why something behaves the way it does. That’s what keeps me digging into data until the story becomes clear.\n\nClarity. Numbers only matter if people can understand them, so I put effort into making my work transparent — whether it’s through well-documented code, clear communication, or turning messy complexity into something others can actually use.\n\nCreativity. Even though I was trained in statistics, I enjoy approaching problems from unexpected angles. Data science, for me, is also a creative act: finding new ways to ask questions and design solutions. In many ways, I bring the same mindset I have as an artist into my work with data.\n\nGrowth. This field never stands still, and neither do I. I’m motivated by the chance to explore new ideas, test emerging tools, and push myself beyond what I already know. Learning isn’t a side activity, it’s at the heart of how I keep evolving.\n\nAdaptability. My background gave me the chance to move across research and applied projects, so I’m comfortable switching between rigor and creativity depending on the context.\n\nLooking ahead, I want to grow in an industry environment where these traits can translate into real impact. For me, the next step is about sharpening my skills in practice, learning from experienced teams, and contributing to work that matters beyond the notebook."
  },
  {
    "objectID": "about.html#my-passion",
    "href": "about.html#my-passion",
    "title": "about",
    "section": " My passion",
    "text": "My passion\nLong before I ever wrote my first line of code, I grew up surrounded by ceramics — a family tradition where patience, detail, and creativity were part of everyday life. For me, clay was never just a material: it was something alive, something that teaches you to slow down, to experiment, and to accept that not everything comes out perfect on the first try.\nOver time I started creating my own pieces, and that experience shaped me just as much as any formal education. Ceramics taught me that beauty and meaning come from process as much as from outcome, and that even small imperfections can add character and authenticity.\nThat artistic background is still a big part of who I am. It keeps me connected to tradition, but also gives me perspective: the ability to see beyond function, to value form and story, and to treat what I build — in any context — as something that should carry both purpose and personality.\nAnd if you ever hire me as a data scientist, there’s a good chance you’ll also get a piece of my artwork on your desk — a small reminder that numbers and creativity aren’t so far apart after all.\n\n Take a look at my work here:\njg1w6f-ii.myshopify.com\n\n\n\n Recognition\nRegional Council of Campania — Award for Excellence in Production, 2021.\n\n\n\n\n Exhibitions of my artworks\nCeramic “Vesuvian cherry tomato horn” on Abramovich’s yacht\nA unique piece displayed on the yacht of Roman Abramovich, former Chelsea president.\n\n\nExhibitions across the Sorrento and Amalfi Peninsula\nCelebrating the traditions of my homeland through ceramic works.\n\n\n\n\n Media\nRegional cooking shows\nMy San Gennaro ceramic sculpture featured on local TV programs dedicated to Campania’s cuisine.\n\n\nFootball talk shows\nBringing the color and symbolism of Naples’ tradition into national football discussions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Emanuele Iaccarino",
    "section": "",
    "text": "Hi there, I’m Emanuele Iaccarino.\nNumbers have always been my way of making sense of the world — turning ideas into structures, spotting patterns in the noise, and breaking problems into steps I could work through. For a long time I saw this as nothing more than my own way of thinking. Only later did I realize that this mindset is exactly what lies at the heart of data science, and that following this path wasn’t really a choice but the most natural direction forward.\nThat realization shaped my journey. What started as curiosity grew into building models, working with messy data, and uncovering the stories hidden inside. It became not just something I enjoyed, but something I wanted to dedicate myself to fully. Data science didn’t arrive in my life as a plan — it arrived as the recognition of what I had been doing all along.\nEven this website reflects that story. I built it with Quarto, the modern framework born from RStudio, where I first learned to bring statistics, code, and storytelling together. Using the same ecosystem here feels natural — a continuation of the roots that shaped me, now powering the place where I share what I’m building today."
  },
  {
    "objectID": "projects/Churn Analysis on Banking Dataset/index.html",
    "href": "projects/Churn Analysis on Banking Dataset/index.html",
    "title": "Churn Analysis on Banking Dataset",
    "section": "",
    "text": "This project develops a customer churn prediction model for the banking sector, aiming to identify the top 10,000 clients most likely to close their accounts.\nMethods\n- Preprocessing: data cleaning, categorical encoding\n- Sampling: cost-sensitive techniques to address class imbalance\n- Boosting algorithms: LightGBM, XGBoost, CatBoost, with ensemble weighting\n- Hyperparameter optimization: automated tuning with Optuna\n- Interpretability: SHAP values for global and local feature importance\nKey results\n- Ensemble boosting models achieved the best performance using a custom Rank Probabilities metric, prioritizing recall of churners\n- Synthetic data improved testing robustness on unseen distributions\n- SHAP analysis revealed key socio-demographic and account features driving churn risk\nView project on GitHub\n\n\n\nBoosting models with interpretability for churn prediction"
  },
  {
    "objectID": "projects/Marijuana Legalization and Opioid Mortality/index.html",
    "href": "projects/Marijuana Legalization and Opioid Mortality/index.html",
    "title": "Marijuana Legalization and Opioid Mortality: A Causal Analysis Using DiD and IV Models",
    "section": "",
    "text": "This project examines the impact of recreational marijuana legalization on drug-related mortality, with a focus on opioids and cocaine.\nThe analysis applies causal inference methods including Difference-in-Differences (DiD), Instrumental Variables (IV, 2SLS & 2SRI), and Event Studies, using state-level panel data from the CDC.\nMethods\n- Panel dataset: Massachusetts (treatment) vs. New Hampshire (control)\n- Data cleaning and imputation via Kalman filtering\n- DiD and Event Study with monthly leads/lags\n- IV estimation to address potential endogeneity\n- Binary Logit/Probit robustness checks\nKey results\n- 📉 Significant decline in cocaine deaths after legalization (DiD & Event Study)\n- ⚖️ No statistically significant effect on opioid mortality\n- ✅ Placebo and robustness checks confirm validity of cocaine result\n- 🌍 Findings highlight implications for European countries facing emerging overdose risks\nView project on GitHub\n\n\n\nCausal econometric analysis of cannabis policy reform and drug-related mortality"
  },
  {
    "objectID": "projects/Snake with Curriculum Learning/index.html",
    "href": "projects/Snake with Curriculum Learning/index.html",
    "title": "Snake with Curriculum Learning",
    "section": "",
    "text": "This project implements a reinforcement learning agent for the classic Snake game using Stable Baselines3.\nA curriculum learning strategy is applied: the agent is trained on smaller, simpler boards before progressing to larger and more complex environments.\nMethods\n- Custom Snake environments in snakeenv.py and snakeenv2.py\n- RL training with Proximal Policy Optimization (PPO)\n- Curriculum schedule: gradually increasing board size and difficulty\n- Logging and monitoring with Stable Baselines3 tools\nKey results\n- Agent successfully learned to play Snake across multiple board sizes\n- Achieved a maximum score of 18 apples eaten in evaluation runs\n- Curriculum learning improved convergence speed and stability compared to direct training on large boards\nView project on GitHub\n\n\n\nReinforcement learning agent trained with curriculum learning to master Snake"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "projects",
    "section": "",
    "text": "Bayesian Survival Analysis of Veteran Lung Cancer Patients\n\n\na Bayesian survival modeling project comparing exponential and Weibull AFT models on the Veteran’s Lung Cancer dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrediction of Medical Care Abandonment in Italy\n\n\na research project using machine learning to identify individuals most at risk of forgoing necessary healthcare services\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarijuana Legalization and Opioid Mortality: A Causal Analysis Using DiD and IV Models\n\n\nEconometric study of the effects of recreational cannabis legalization on opioid and cocaine mortality, using Difference-in-Differences, IV, and Event Studies.\n\n\n\n\n\n\nJun 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nChurn Analysis on Banking Dataset\n\n\nPredicting customer churn in the banking sector using boosting algorithms, Optuna optimization, and interpretable ML techniques.\n\n\n\n\n\n\nMay 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSQL Parser Data Pipeline\n\n\nA Python library for parsing and interpreting complex SQL queries, designed for BigQuery workflows and adaptable to other SQL dialects.\n\n\n\n\n\n\nApr 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunity Detection Algorithms — Self-Implemented and Optimized\n\n\nA Python library implementing and optimizing community detection algorithms (Girvan-Newman, Spectral Clustering, Louvain) with significant performance improvements over…\n\n\n\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSnake with Curriculum Learning\n\n\nTraining an RL agent to play Snake using Stable Baselines3, with curriculum learning to progressively increase difficulty.\n\n\n\n\n\n\nSep 1, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pubblications/Breast Cancer/index.html",
    "href": "pubblications/Breast Cancer/index.html",
    "title": "Breast Cancer Classification with Deep Learning",
    "section": "",
    "text": "This project develops a deep learning pipeline for mammogram classification using the CBIS-DDSM dataset, addressing preprocessing, augmentation, and model training challenges in medical imaging.\nTechniques include ROI extraction, intensity maps, gradient transformations, and custom data augmentation (ROI overlay, geometric transformations, blended cropping). Pretrained CNNs were fine-tuned through transfer learning, achieving state-of-the-art performance among public baselines.\nKey results:\n- Preprocessing improved convergence and accuracy\n- Transfer learning boosted generalization performance\n- Augmentation strategies reduced overfitting\n- Demonstrated potential for scalable and reproducible breast cancer diagnostics\nRead full-text on ResearchGate\n\n\n\nDeep learning for mammogram classification using the CBIS dataset"
  },
  {
    "objectID": "pubblications/Unity ML Agents/index.html",
    "href": "pubblications/Unity ML Agents/index.html",
    "title": "Unity ML Agents: Wall Jump and SoccerTwos Environment using Reinforcement Learning Techniques",
    "section": "",
    "text": "This study applies reinforcement learning (RL) to the Unity ML-Agents Toolkit, with case studies on WallJump and SoccerTwos.\nAlgorithms implemented include Imitation Learning, Proximal Policy Optimization (PPO), Soft Actor-Critic (SAC), and POCA, evaluated with TensorBoard monitoring.\nEnhancements explored:\n- Curriculum learning in WallJump for complex task training\n- Self-play in SoccerTwos for competitive multi-agent strategies\n- Hyperparameter tuning for improved stability and performance\nKey results:\n- SAC outperformed PPO in continuous-control environments (3DBall)\n- Curriculum learning accelerated convergence in WallJump\n- Self-play produced stronger agents in adversarial settings\n- Hyperparameter tuning critically influenced model robustness\nRead on ResearchGate\n\n\n\nUnity ML-Agents environments explored: WallJump and SoccerTwos"
  }
]